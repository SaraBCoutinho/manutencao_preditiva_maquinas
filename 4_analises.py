# -*- coding: utf-8 -*-
"""Desafio_ciclo2_4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YR3EGlztuiPntTVJiXB-MMZy1GxL4d6c
"""

#imports
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris, make_multilabel_classification
from scipy import stats
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.metrics import roc_curve, auc
from joblib import dump, load

#unzip files
!unzip -q /content/model_evaluation_results.zip

#from google.colab import files
#uploaded = files.upload()

#dados de resultados
import pickle
file1 = "/content/model_evaluation_results/resultados_treina.pkl"
file2 = "/content/model_evaluation_results/resultados_treina_multi.pkl"
with open(file1, "rb") as f:
    dados_resultados = pickle.load(f)
with open(file2, "rb") as f:
    dados_resultados_multi = pickle.load(f)

#Foram adotados cenários de predições distintas dos quais considerou-se os seguintes contextos:
# - (2) normalizado e não normalizado
# - (2) com valores discrepantes e valores discrepantes tratados
# - (3) modelos distintos adotados
# - (2) dados classificação binária e multilabel
# - (2) dados balanceados e não balanceados
# - (3) foram feitas 3 iterações para cada tipo de classificação

!mkdir figuras_predicoes/

#--Funções
def plot_metric(dataframe, metric_name):
    plt.figure(figsize=(12, 6))
    sns.barplot(x=dataframe.index, y=metric_name, data=dataframe)
    plt.title(f'Comparação das métricas {metric_name.replace("_", " ").title()} por abordagem')
    plt.ylabel(metric_name.replace("_", " ").title())
    plt.xlabel('Abordagem')
    plt.xticks(rotation=90)
    plt.tight_layout()
    plt.show()
    plt.savefig(f'/content/figuras_predicoes/comparacao_metricas_{metric_name}.pdf')

def plot_metric_multi(dataframe, metric_name):
    plt.figure(figsize=(12, 6))
    sns.barplot(x=dataframe.index, y=metric_name, data=dataframe, color='g')
    plt.title(f'Comparação das métricas {metric_name.replace("_", " ").title()} por abordagem')
    plt.ylabel(metric_name.replace("_", " ").title())
    plt.xlabel('Abordagem - Multi')
    plt.xticks(rotation=90)
    plt.tight_layout()
    plt.show()
    plt.savefig(f'/content/figuras_predicoes/comparacao_metricas_multi_{metric_name}.pdf')

#resultados das métricas
resultados = dados_resultados[0].get('metrics')
resultados_multi = dados_resultados_multi[0].get('metrics')

#plot métricas por abordagem classificação binária
metrics_df = pd.DataFrame(resultados).T
plot_metric(metrics_df, 'accuracy')
plot_metric(metrics_df, 'precision')
plot_metric(metrics_df, 'recall')
plot_metric(metrics_df, 'f1_score')

#plot métricas por abordagem classificação multilabel
metrics_df_multi = pd.DataFrame(resultados_multi).T
plot_metric_multi(metrics_df_multi, 'accuracy')
plot_metric_multi(metrics_df_multi, 'precision')
plot_metric_multi(metrics_df_multi, 'recall')
plot_metric_multi(metrics_df_multi, 'f1_score')

#analisando os maiores resultados para cada métrica na classificação binária e multilabel
#Por ser um problema de manutenção, escolheu-se dentre essas métricas focar na recall.
metrics = ['recall'] #['accuracy', 'precision', 'recall', 'f1_score']

print(f"--Classificação binária--")
metrics_df = pd.DataFrame(resultados).T
for metric in metrics:
    if metric in metrics_df.columns:
        max_value = metrics_df[metric].max()
        best_models = metrics_df[metrics_df[metric] == max_value].index.tolist()
        print(f"Best {metric.replace('_', ' ').title()}:")
        print(f"  Value: {max_value}")
        print(f"  Models: {best_models}")
        print("-" * 30)

print(f"--Classificação multilabel--")
metrics_df_multi = pd.DataFrame(resultados_multi).T
for metric in metrics:
    if metric in metrics_df.columns:
        max_value = metrics_df_multi[metric].max()
        best_models = metrics_df_multi[metrics_df_multi[metric] == max_value].index.tolist()
        print(f"Best {metric.replace('_', ' ').title()}:")
        print(f"  Value: {max_value}")
        print(f"  Models: {best_models}")
        print("-" * 30)

#ambos os casos foram obtidos pelo Random Forest (RF)
#Na classificação binária o RF obteve o mesmo desempenho com dados que passaram pela técnica de scaler
# ou não e que tiveram valores discrepantes ou não
#o resultado obtido foi de 99,8% de recall
#para a classificação multilabel era esperado um resultado menor visto que existem mais classes e a probabilidade de acerto é menor
#para um baseline de classificador aleatório.
#nesse caso o RF obteve o mesmo resultado para os dados com ou sem os valores discrepantes tratados, mas aplicar a técnica de scaler fez diferença
#o resultato de recall obtido foi de 23%
#o modelo final escolhido como proposta foi o 'RF-BALANCED_df_treated_scaled' e para o multi foi 'RF-BALANCED_multi_df_treated_scaled'

#ambos os casos foram obtidos pelo Random Forest (RF)
#Na classificação binária o RF obteve o mesmo desempenho com dados que passaram pela técnica de scaler
# ou não e que tiveram valores discrepantes ou não
#o resultado obtido foi de 99,8% de recall
#para a classificação multilabel era esperado um resultado menor visto que existem mais classes e a probabilidade de acerto é menor
#para um baseline de classificador aleatório.
#nesse caso o RF obteve o mesmo resultado para os dados com ou sem os valores discrepantes tratados, mas aplicar a técnica de scaler fez diferença
#o resultato de recall obtido foi de 23%
#o modelo final escolhido como proposta foi o 'RF-BALANCED_df_treated_scaled' e para o multi foi 'RF-BALANCED_multi_df_treated_scaled'

#conforme visto na etapa anterior, o RF é um modelo que no entanto pode levar a um maior tempo de treinamento.
#apesar disso, este modelo foi escolhido
#outro ponto importante é que foram adotados os pesos balanceados já que se trata de um conjunto de dados desbalanceado, seja no cenário
#de 2 ou mais classes.

#para predizer qual falha ocorreu, vamos focar no conjunto multilabel
#nesse conjunto vemos qual das componentes da label tem uma probabilidade maior de ocorrência.
#usaremos o padrão considerado no conjunto de dados e os nomes das colunas conforme foram renomeados
#seguindo a ordem das colunas no dataframe:
# - FDF(Falha Desgaste Ferramenta) : label_0
# - FDC  (Falha Dissipacao Calor) : label_1
# - FP (Falha Potencia: label_2
# - FTE (Falha Tensao Excessiva): label_3
# - FAL (Falha Aleatoria): label_4

!unzip saved_models.zip

model =  load('/content/saved_models/modelo_multi_RF-BALANCED_df_treated_scaled.joblib')

#o modelo final adotado no sistema:

model

#y_test = pd.read_csv('/content/bootcamp_train_2_df_treated_2.csv')
#y_test = y_test[['FDF','FDC','FP','FTE','FAL']]
#colunas_labels = ['FDF','FDC','FP','FTE','FAL']
#y_test["labels"] = y_test[colunas_labels].apply(lambda row: list(row.values),axis=1)
#y_test = y_test['labels']

model_name = 'RF-BALANCED_multi_df_treated_scaled'
model_results = None

# Find the results for the specified model in the first iteration
if dados_resultados_multi and isinstance(dados_resultados_multi[0], dict):
    first_iteration_results = dados_resultados_multi[0]
    if 'metrics' in first_iteration_results and model_name in first_iteration_results['metrics']:
        model_results = {
            'metrics': first_iteration_results['metrics'].get(model_name),
            'predictions': first_iteration_results['predictions'].get(model_name),
            'proba': first_iteration_results['proba'].get(model_name)
        }

if model_results:
    print(f"Results for model: {model_name} (from first iteration)")
    display(model_results)
else:
    print(f"Results for model {model_name} not found in the first iteration of dados_resultados_multi.")

#nos dados a label com maior probabilidade de ocorrência é a label_1 (Falha por Dissipação de Calor)
#e a menor é a label_4 (Falha aleatória)

resultados_multi.get('RF-BALANCED_multi_df_treated_scaled')

nomes = ['FDF','FDC','FP','FTE','FAL']
valores =  [np.float64(0.7510674210726512),np.float64(0.9410007069561575),np.float64(0.8981230158730159),np.float64(0.9345750301932367),np.float64(0.4669746043819062)]

plt.figure(figsize=(10, 6))
sns.barplot(x=nomes, y=valores)
plt.title('Probabilidades por Tipo de Falha')
plt.xlabel('Tipo de falha')
plt.ylabel('Probabilidade')
plt.ylim(0, 1)
plt.show()
plt.savefig('/content/figuras_predicoes/probabilidades_por_tipo_de_falha.pdf')

model_name = 'RF-BALANCED_multi_df_treated_scaled'
probabilities = None

if dados_resultados_multi and isinstance(dados_resultados_multi[0], dict):
    first_iteration_results = dados_resultados_multi[0]
    if 'proba' in first_iteration_results and model_name in first_iteration_results['proba']:
        probabilities = first_iteration_results['proba'].get(model_name)

if probabilities is not None:
    print(f"Tipo de Falha com maior probabilidade: {model_name}")

    if isinstance(probabilities, list) and all(isinstance(arr, np.ndarray) for arr in probabilities):
        num_samples = len(probabilities[0]) if len(probabilities) > 0 else 0
        num_labels = len(probabilities)

        for i in range(num_samples):
            max_proba = -1
            max_label = -1
            sample_probabilities = []
            for j in range(num_labels):
                 if i < len(probabilities[j]):
                    proba_class_1 = probabilities[j][i][1]
                    sample_probabilities.append(f"Label {j}: {proba_class_1:.4f}")
                    if proba_class_1 > max_proba:
                        max_proba = proba_class_1
                        max_label = j
                 else:
                    sample_probabilities.append(f"Tipo de Falha {j}: N/A")


            if max_label != -1:
                print(f"  Sample {i}: Maior probabilidade por tipo de falha {max_label} ({max_proba:.4f}). Probabilidades e predições: [{', '.join(sample_probabilities)}]")

#download zip imagens
!zip -r figuras_predicoes.zip figuras_predicoes/

