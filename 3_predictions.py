# -*- coding: utf-8 -*-
"""Desafio_ciclo2_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PStyWcONlve8dBJYNAfXzCq7zjEFtgNu
"""

#--Preparação do ambiente
!pip install scikit-learn imbalanced-learn
#imports
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.multioutput import MultiOutputClassifier
#from sklearn.svm import SVC
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
#from sklearn.datasets import load_iris, make_multilabel_classification
from scipy import stats
from sklearn.preprocessing import QuantileTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression, SGDOneClassSVM
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from collections import Counter
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc
import time
from sklearn.multiclass import OneVsRestClassifier # Import OneVsRestClassifier
import os
import pickle
from joblib import dump, load

#--Data
from google.colab import files
uploaded = files.upload()
file1 = '/content/bootcamp_train_2_df_2.csv'
file2 = '/content/bootcamp_train_2_df_treated_2.csv'
df = pd.read_csv(file1)
df_treated = pd.read_csv(file2)

#Classificação binária
df = df.drop(columns=['Unnamed: 0'])
#outliers tratados
df_treated = df_treated.drop(columns=['Unnamed: 0'])

#--Balanceamento
smote = SMOTE(sampling_strategy='minority', random_state=42)

#--Scaler
scaler = QuantileTransformer(output_distribution="normal", random_state=42)

#grid search e tempo de treinamento
X_grid, y_grid = df_treated.drop(columns=['F','FDF', 'FDC', 'FP', 'FTE', 'FAL']), df['F']

#lr-ovr
param_grid_lr = {'estimator__C': [0.001, 0.1, 10]}
grid_lr = GridSearchCV(OneVsRestClassifier(LogisticRegression(solver='liblinear', random_state=42)),param_grid_lr,cv=3,scoring="f1_macro",n_jobs=-1)
start_time = time.time()
grid_lr.fit(X_grid, y_grid)
end_time = time.time()
training_time = end_time - start_time
print("Tempo de treinamento LR-OVR:", training_time, "segundos")
print("Melhores parâmetros LR-OVR:", grid_lr.best_params_)
print("Melhor score LR-OVR:", grid_lr.best_score_)

#rf
param_grid_rf = {"n_estimators": [10, 50], "max_depth": [10, 20],}
grid_rf = GridSearchCV(RandomForestClassifier(random_state=42),param_grid_rf,cv=3,scoring="f1_macro",n_jobs=-1)
start_time = time.time()
grid_rf.fit(X_grid, y_grid)
end_time = time.time()
training_time = end_time - start_time
print("Tempo de treinamento RF:", training_time, "segundos")
print("Melhores parâmetros RF:", grid_rf.best_params_)
print("Melhor score RF:", grid_rf.best_score_)

#ada
param_grid_ada = {'n_estimators': [10, 50], 'learning_rate': [0.01, 0.1]}
grid_ada = GridSearchCV(AdaBoostClassifier(random_state=42),param_grid_ada,cv=3,scoring="f1_macro",n_jobs=-1)
start_time = time.time()
grid_ada.fit(X_grid, y_grid)
end_time = time.time()
training_time = end_time - start_time
print("Tempo de treinamento ADA:", training_time, "segundos")
print("Melhores parâmetros ADA:", grid_ada.best_params_)
print("Melhor score ADA:", grid_ada.best_score_)

#na iteração feita os tempos de treinamento capturados foram:
#Tempo de treinamento LR-OVR: 4.759691953659058 segundos
#Tempo de treinamento RF: 14.531857013702393 segundos
#Tempo de treinamento ADA: 6.43994665145874 segundos
#Os tempos de treinamento em ordem crescente foram para os modelos [LR-OVR, ADA, RF] adotando dados com valores discrepantes tratados e classificação binária

#--Modelos (LR-OVR, RF-BALANCED, ADA+SMOTE)
#capturando os melhores parâmetros para adotar nos modelos
best_params_lr = grid_lr.best_params_
best_params_rf = grid_rf.best_params_
best_params_ada = grid_ada.best_params_

best_params_lr_multi = {k.replace('estimator__', ''): v for k, v in best_params_lr.items()}
best_params_rf_multi = best_params_rf
best_params_ada_multi = best_params_ada

#classificação binária
models = {
    'LR-OVR':  OneVsRestClassifier(LogisticRegression(solver='liblinear', random_state=42, **best_params_lr_multi)),
    'RF-BALANCED': RandomForestClassifier(class_weight='balanced', random_state=42, **best_params_rf),
    'ADA': AdaBoostClassifier(random_state=42, **best_params_ada)
}

#classificação multilabel
models_multi = {
    'LR-OVR': MultiOutputClassifier(LogisticRegression(solver='liblinear', random_state=42, **best_params_lr_multi)),
    'RF-BALANCED': MultiOutputClassifier(RandomForestClassifier(class_weight='balanced', random_state=42, **best_params_rf_multi)),
    'ADA': MultiOutputClassifier(AdaBoostClassifier(random_state=42, **best_params_ada_multi)),
}

!mkdir saved_models

#--Funções


def treina(df, model=models['LR-OVR'], escala=True):
  """Função para treinar os modelos - 2 classes"""

  X = df.drop(columns=['F'])
  y = df['F']

  #balanceamento - SMOTE
  if model == models['ADA']:
     smote = SMOTE(sampling_strategy='minority', random_state=42)
     X_res, y_res = smote.fit_resample(X, y)
     X, y = X_res, y_res

  # Train-test split with stratify for binary/multi-class
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42, stratify=y)

  if escala == True:
    pipeline = Pipeline([('scaler', scaler), ('classifier', model)])
    pipeline.fit(X_train,y_train)
    predictions = pipeline.predict(X_test)
    scores = None
    if hasattr(pipeline, "predict_proba"):
      scores = pipeline.predict_proba(X_test)
    return predictions, scores, y_test, pipeline
  else:
    model.fit(X_train,y_train)
    predictions = model.predict(X_test)
    scores = None
    if hasattr(model, "predict_proba"):
      scores = model.predict_proba(X_test)
    return predictions, scores, y_test, model

def treina_multi(df, model=models_multi['LR-OVR'], escala=True):
  """Função para treinar os modelos - Multilabel"""

  colunas_labels = ['FDF','FDC','FP','FTE','FAL']

  X = df.drop(columns=['F'] + colunas_labels)
  y = df[colunas_labels]

  #balanceamento - SMOTE
  if model == models['ADA']:
    y_res_list = []
    X_res_list = []

    for col in y.columns:
      smote = SMOTE(sampling_strategy='minority', random_state=42)
      X_res_col, y_res_col = smote.fit_resample(X, y[col])
      X_res_list.append(X_res_col)
      y_res_list.append(y_res_col)
    X, y = X_res_list, y_res_list

  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)

  if escala == True:
    pipeline = Pipeline([('scaler', scaler), ('classifier', model)])
    pipeline.fit(X_train,y_train)
    predictions = pipeline.predict(X_test)
    scores = None
    if hasattr(pipeline.named_steps['classifier'].estimator, "predict_proba"):
       scores = pipeline.predict_proba(X_test)
    return predictions, scores, y_test, pipeline
  else:
    model.fit(X_train,y_train)
    predictions = model.predict(X_test)
    scores = None
    if hasattr(model, "predict_proba"):
      scores = model.predict_proba(X_test)
    return predictions, scores, y_test, model

#all_iterations_results = []
all_results = []
#all_iterations_results_multi = []

#num_iterations = 3

#predições classificação binária

#for i in range(num_iterations):
#    print(f"--- Iteração {i+1}/{num_iterations} ---")

#inicializando dicionario dos resultados
dict_results_predictions = {}
dict_results_proba = {}
dict_results_metrics = {}

#aplicando função classificação binária
for model_name in models.keys():
  for data, df_name in [(df, "df"), (df_treated, "df_treated")]:
    for escala, escala_name in [(True, "scaled"), (False, "not_scaled")]:
          print(f"--Modelo {model_name}--Dataframe {df_name}--Escala {escala_name}--")
          chave = f"{model_name}_{df_name}_{escala_name}"

          try:
              y_pred, y_prob, y_test, model = treina(data, model=models[model_name], escala=escala)

              # Salvar modelo
              filename = f'saved_models/modelo_{model_name}_{df_name}_{escala_name}.joblib'
              dump(model, filename)
              #filename = f'saved_models/modelo_iteracao_{i}_{model_name}_{df_name}_{escala_name}.pkl'  # nome iterativo
              #with open(filename, 'wb') as f:
              #  pickle.dump(model, f)

              #cálculo das métricas
              accuracy = accuracy_score(y_test, y_pred)
              precision = precision_score(y_test, y_pred, average='weighted')
              recall = recall_score(y_test, y_pred, average='weighted')
              f1 = f1_score(y_test, y_pred, average='weighted')

              roc_auc_scores = {}
              if y_prob is not None:
                  for i in range(y_prob.shape[1]):
                      try:
                          y_test_bin = (y_test == i).astype(int)
                          y_prob_class = y_prob[:, i]
                          fpr, tpr, thresholds = roc_curve(y_test_bin, y_prob_class)
                          roc_auc = auc(fpr, tpr)
                          roc_auc_scores[f'class_{i}'] = roc_auc
                      except ValueError as e:
                          print(f"Não é possível calcular a ROC AUC para classe {i}: {e}")
                          roc_auc_scores[f'class_{i}'] = None
              else:
                  print(f"Probabilidades não disponíveis para o modelo {model_name}.")
                  roc_auc_scores = None
              dict_results_predictions[chave] = y_pred
              dict_results_proba[chave] = y_prob
              dict_results_metrics[chave] = {'accuracy': accuracy,'precision': precision,'recall': recall,'f1_score': f1,'roc_auc': roc_auc_scores}

          except Exception as e:
              print(f"An error occurred for {chave}: {e}")
              dict_results_predictions[chave] = None
              dict_results_proba[chave] = None
              dict_results_metrics[chave] = None

#guardando todos os resultados
#all_iterations_results.append({
all_results.append({
    #'iteration': i,
    'predictions': dict_results_predictions,
    'proba': dict_results_proba,
    'metrics': dict_results_metrics,
})

#print("\n--- Predições geradas para todas as iterações - Classificação binária ---")
print("\n--- Predições geradas - Classificação binária ---")

#predições classificação multilabel
all_results_multi = []
#for i in range(num_iterations):
#    print(f"--- Iteração {i+1}/{num_iterations} ---")

#inicializando dicionario dos resultados
dict_results_predictions_multi = {}
dict_results_proba_multi = {}
dict_results_metrics_multi = {}

#aplicando função classificação multilabel
for model_name in models_multi.keys():
  for data, df_name in [(df, "df"), (df_treated, "df_treated")]:
    for escala, escala_name in [(True, "scaled"), (False, "not_scaled")]:
          print(f"--Modelo {model_name}--Dataframe {df_name}--Escala {escala_name}--")
          chave = f"{model_name}_multi_{df_name}_{escala_name}"

          try:
              y_pred, y_prob, y_test, model= treina_multi(data, model=models_multi[model_name], escala=escala)

              # Salvar modelo
              filename = f'saved_models/modelo_multi_{model_name}_{df_name}_{escala_name}.joblib'
              dump(model, filename)
#                  filename = f'saved_models/modelo_multi_iteracao_{i}_{model_name}_{df_name}_{escala_name}.pkl'  # nome iterativo
#                  with open(filename, 'wb') as f:
#                    pickle.dump(model, f)

              #cálculo das métricas
              accuracy = accuracy_score(y_test, y_pred)
              precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)
              recall = recall_score(y_test, y_pred, average='weighted', zero_division=1)
              f1 = f1_score(y_test, y_pred, average='weighted', zero_division=1)

              roc_auc_scores = {}
              if y_prob is not None and isinstance(y_prob, list):
                  for i in range(len(y_prob)):
                      try:
                          y_test_bin = y_test.iloc[:, i]
                          y_prob_class = y_prob[i][:, 1]
                          fpr, tpr, thresholds = roc_curve(y_test_bin, y_prob_class)
                          roc_auc = auc(fpr, tpr)
                          roc_auc_scores[f'label_{i}'] = roc_auc
                      except ValueError as e:
                          print(f"Não é possível calcular a ROC AUC para classe {i}: {e}")
                          roc_auc_scores[f'label_{i}'] = None
                      except IndexError as e:
                            print(f"Index error.")
                            roc_auc_scores[f'label_{i}'] = None
              else:
                  print(f"Erro ao buscar as probabilidades para o {model_name}.")
                  roc_auc_scores = None

              dict_results_predictions_multi[chave] = y_pred
              dict_results_proba_multi[chave] = y_prob
              dict_results_metrics_multi[chave] = {'accuracy': accuracy,'precision': precision,'recall': recall,'f1_score': f1,'roc_auc': roc_auc_scores}

          except Exception as e:
              print(f"An error occurred for {chave}: {e}")
              dict_results_predictions_multi[chave] = None
              dict_results_proba_multi[chave] = None
              dict_results_metrics_multi[chave] = None

#guardando todos os resultados
#all_iterations_results_multi.append({
all_results_multi.append({
    #'iteration': i,
    'predictions': dict_results_predictions_multi,
    'proba': dict_results_proba_multi,
    'metrics': dict_results_metrics_multi,
})

#print("\n--- Predições geradas para todas as iterações - Classificação multilabel ---")
print("\n--- Predições geradas - Classificação multilabel ---")

#salvando resultados

results_dir = "model_evaluation_results"
os.makedirs(results_dir, exist_ok=True)

resultados_treina = os.path.join(results_dir, "resultados_treina.pkl")
resultados_treina_multi = os.path.join(results_dir, "resultados_treina_multi.pkl")

with open(resultados_treina, 'wb') as f:
    pickle.dump(all_results, f)
    #pickle.dump(all_iterations_results, f)

with open(resultados_treina_multi, 'wb') as f:
    pickle.dump(all_results_multi, f)
    #pickle.dump(all_iterations_results_multi, f)

print("-"*50)
print(f"Results saved to '{results_dir}' directory.")

#download zip imagens
!zip -r model_evaluation_results.zip model_evaluation_results/

#download zip imagens
!zip -r saved_models.zip saved_models/

